{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import ssl # Quickfix to torchaudio ssl error\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    image = cv2.resize(image, (224, 224))  # Resize to 224x224\n",
    "    image = image / 255.0  # Normalize to [0, 1]\n",
    "    image = np.transpose(image, (2, 0, 1))  # Convert to (C, H, W)\n",
    "    image = torch.tensor(image, dtype=torch.float32)\n",
    "    return image\n",
    "\n",
    "def show_image(dataloader, index):\n",
    "    # Get a batch of data\n",
    "    data_iter = iter(dataloader)\n",
    "    images, labels = next(data_iter)\n",
    "\n",
    "    # Ensure the index is within the batch size\n",
    "    batch_size = images.size(0)\n",
    "    if index >= batch_size:\n",
    "        raise IndexError(f\"Index {index} is out of bounds for batch size {batch_size}\")\n",
    "\n",
    "    # Get the image and label at the specified index within the batch\n",
    "    image = images[index]\n",
    "    label = labels[index]\n",
    "\n",
    "    # If images were normalized, we might need to denormalize them\n",
    "    # For example, if we used transforms.Normalize(mean, std), we need to unnormalize\n",
    "    # Replace these mean and std values with those used in your transforms\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])\n",
    "    image = image * std[:, None, None] + mean[:, None, None]\n",
    "\n",
    "    # Convert tensor to numpy array\n",
    "    image_np = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    # Clip values to [0,1] if necessary\n",
    "    image_np = np.clip(image_np, 0, 1)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.title(f\"Label: {label.item()}\")  # Use label name if available\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis('off')  # Hide axis ticks\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "class Resize:\n",
    "    def __init__(self, size):\n",
    "        self.size = size  # (h, w)\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image = F.interpolate(image.unsqueeze(0), size=self.size, mode='bilinear', align_corners=False)\n",
    "        return image.squeeze(0)\n",
    "\n",
    "class SkinDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.label_to_index = {}\n",
    "        \n",
    "        self._build_label_index()\n",
    "\n",
    "    def _build_label_index(self):\n",
    "        label_names = sorted([\n",
    "            d for d in os.listdir(self.root_dir)\n",
    "            if os.path.isdir(os.path.join(self.root_dir, d))\n",
    "        ])\n",
    "        \n",
    "        self.label_to_index = {label_name: idx for idx, label_name in enumerate(label_names)}\n",
    "        \n",
    "        for label_name in label_names:\n",
    "            label_dir = os.path.join(self.root_dir, label_name)\n",
    "            label_index = self.label_to_index[label_name]\n",
    "            for img_name in os.listdir(label_dir):\n",
    "                img_path = os.path.join(label_dir, img_name)\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.labels.append(label_index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Failed to load image at path: {img_path}\")\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0  # Shape: (C, H, W)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = F.interpolate(image.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Chickenpox': 0, 'Cowpox': 1, 'HFMD': 2, 'Healthy': 3, 'Measles': 4, 'Monkeypox': 5}\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    Resize((224, 224)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = SkinDataset(root_dir='./dataset/Train/', transform=data_transforms)\n",
    "test_dataset = SkinDataset(root_dir='./dataset/Test/', transform=data_transforms)\n",
    "valid_dataset = SkinDataset(root_dir='./dataset/Valid/', transform=data_transforms)\n",
    "\n",
    "print(train_dataset.label_to_index)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV3Model(nn.Module):\n",
    "    def __init__(self, num_classes, extractor_trainable: bool = True):\n",
    "        super(MobileNetV3Model, self).__init__()\n",
    "        mobilenet = models.mobilenet_v3_large(pretrained=True)\n",
    "        \n",
    "        self.feature_extractor = mobilenet.features\n",
    "        \n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = extractor_trainable\n",
    "        \n",
    "        self.out_features = mobilenet.classifier[0].in_features\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.out_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        \n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(x.size(0), -1)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetModel(nn.Module):\n",
    "    def __init__(self, num_classes, extractor_trainable=True):\n",
    "        super(ResNetModel, self).__init__()\n",
    "        \n",
    "        # Load the pre-trained ResNet model\n",
    "        resnet = models.resnet34(pretrained=True)  # You can also choose resnet18, resnet34, resnet101, etc.\n",
    "        \n",
    "        # Freeze the feature extractor part if extractor_trainable is False\n",
    "        if not extractor_trainable:\n",
    "            for param in resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Replace the final fully connected layer (ResNet's classifier) to match num_classes\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])  # Remove the last fc layer\n",
    "        \n",
    "        # Get the number of input features of the final layer\n",
    "        num_features = resnet.fc.in_features\n",
    "        \n",
    "        # Define the new fully connected layer for classification\n",
    "        self.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the feature extractor (ResNet backbone)\n",
    "        x = self.feature_extractor(x)\n",
    "        \n",
    "        # Flatten the output from the ResNet backbone\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # Pass the flattened features through the classifier\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, epochs, optimizer, loss_fn, data_loader, device, fold=0):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(data_loader, total=len(data_loader), leave=False)\n",
    "        model.train()\n",
    "        mean_loss = 0\n",
    "\n",
    "        for _, (X, y) in enumerate(loop):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            pred = model(X)\n",
    "            \n",
    "            loss = loss_fn(pred, y)\n",
    "            mean_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loop.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "        mean_loss /= len(data_loader)\n",
    "        epoch_losses.append(mean_loss)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] completed. Avg loss: {mean_loss:.4f}\")\n",
    "        \n",
    "    print(f\"Training fold {fold+1} completed.\")\n",
    "    \n",
    "    return epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(model, loss_fn, data_loader, device):\n",
    "    model.eval()\n",
    "    size = len(data_loader.dataset)\n",
    "    num_batches = len(data_loader)\n",
    "    test_loss, correct = 0.0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(X)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(outputs, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Get predicted classes\n",
    "            _, pred_labels = torch.max(outputs, 1)\n",
    "\n",
    "            # Calculate number of correct predictions\n",
    "            correct += (pred_labels == y).sum().item()\n",
    "\n",
    "            # Move tensors to CPU and convert to numpy arrays\n",
    "            pred_labels = pred_labels.cpu().numpy()\n",
    "            y = y.cpu().numpy()\n",
    "\n",
    "            # Store predictions and true labels for metrics\n",
    "            all_preds.extend(pred_labels)\n",
    "            all_labels.extend(y)\n",
    "\n",
    "    # Average loss and accuracy\n",
    "    test_loss /= num_batches\n",
    "    accuracy = (correct / size) * 100\n",
    "\n",
    "    print(f\"Validation Error:\\n Accuracy: {accuracy:.2f}%, Avg loss: {test_loss:.4f}\\n\")\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Calculate F-beta score with beta=2\n",
    "    fbeta = fbeta_score(all_labels, all_preds, beta=2, average='weighted')\n",
    "\n",
    "    print(f\"F-beta Score (beta=2): {fbeta:.4f}\\n\")\n",
    "\n",
    "    return conf_matrix, accuracy, (all_labels, all_preds), fbeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(dataset):\n",
    "    \"\"\"\n",
    "    Calculate class weights based on the frequency of each class in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: A PyTorch dataset (e.g., DiabeticDataset).\n",
    "        \n",
    "    Returns:\n",
    "        class_weights: A tensor of class weights to be used in the loss function.\n",
    "    \"\"\"\n",
    "    # Get the labels from the dataset\n",
    "    labels = [label for _, label in dataset]\n",
    "\n",
    "    # Count the frequency of each class\n",
    "    class_counts = np.bincount(labels)\n",
    "    \n",
    "    # Calculate weights as the inverse of the frequency of each class\n",
    "    class_weights = 1.0 / class_counts\n",
    "    \n",
    "    # Normalize the weights to ensure stability\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "    # Convert the weights to a PyTorch tensor\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "# Usage:\n",
    "# Calculate class weights based on the training dataset\n",
    "class_weights = calculate_class_weights(train_dataset)\n",
    "\n",
    "# Move the class weights to the appropriate device\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# Define the loss function with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] completed. Avg loss: 1.5062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] completed. Avg loss: 1.1567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] completed. Avg loss: 0.9042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] completed. Avg loss: 0.6431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] completed. Avg loss: 0.5565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] completed. Avg loss: 0.5440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] completed. Avg loss: 0.5094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] completed. Avg loss: 0.3289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] completed. Avg loss: 0.2431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] completed. Avg loss: 0.1824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] completed. Avg loss: 0.2265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] completed. Avg loss: 0.1658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] completed. Avg loss: 0.1839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] completed. Avg loss: 0.1521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] completed. Avg loss: 0.2057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] completed. Avg loss: 0.1998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] completed. Avg loss: 0.1273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] completed. Avg loss: 0.1230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] completed. Avg loss: 0.0965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20] completed. Avg loss: 0.0768\n",
      "Training fold 1 completed.\n",
      "Validation Error:\n",
      " Accuracy: 80.56%, Avg loss: 1.0073\n",
      "\n",
      "F-beta Score (beta=2): 0.8037\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = ResNetModel(6).to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 20\n",
    "\n",
    "# Train the model\n",
    "train_losses = training_loop(\n",
    "    model=model, \n",
    "    epochs=num_epochs, \n",
    "    optimizer=optimizer, \n",
    "    loss_fn=criterion, \n",
    "    data_loader=train_loader, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "# After training, validate the model\n",
    "conf_matrix, val_accuracy, (all_labels, all_preds), fbeta = validation_loop(\n",
    "    model, criterion, valid_loader, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'resnet_weights.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
